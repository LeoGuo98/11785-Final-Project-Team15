{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGNN-attentionloss.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHdB2npMPeWE"
      },
      "source": [
        "import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GNN(Module):\n",
        "    def __init__(self, hidden_size, step=1):\n",
        "        super(GNN, self).__init__()\n",
        "        self.step = step\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = hidden_size * 2\n",
        "        self.gate_size = 3 * hidden_size\n",
        "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
        "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
        "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
        "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
        "\n",
        "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "\n",
        "    def GNNCell(self, A, hidden):\n",
        "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
        "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
        "        inputs = torch.cat([input_in, input_out], 2)\n",
        "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
        "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
        "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
        "        resetgate = torch.sigmoid(i_r + h_r)\n",
        "        inputgate = torch.sigmoid(i_i + h_i)\n",
        "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        return hy\n",
        "\n",
        "    def forward(self, A, hidden):\n",
        "        for i in range(self.step):\n",
        "            hidden = self.GNNCell(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class SessionGraph(Module):\n",
        "    def __init__(self, opt, n_node, n_side):\n",
        "        super(SessionGraph, self).__init__()\n",
        "        self.hidden_size = opt.hiddenSize\n",
        "        self.n_node = n_node\n",
        "        self.n_side = n_side\n",
        "        self.batch_size = opt.batchSize\n",
        "        self.nonhybrid = opt.nonhybrid\n",
        "        print(self.n_node)\n",
        "        print(self.n_side)\n",
        "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
        "        self.gnn = GNN(self.hidden_size, step=opt.step)\n",
        "        self.embedding_side = nn.Embedding(self.n_side, self.hidden_size)\n",
        "        self.gnn_side = GNN(self.hidden_size, step=opt.step)\n",
        "        self.linear_one = nn.Linear(self.hidden_size * 2, self.hidden_size * 2, bias=True)\n",
        "        self.linear_two = nn.Linear(self.hidden_size * 2, self.hidden_size * 2, bias=True)\n",
        "        self.linear_three = nn.Linear(self.hidden_size * 2, 1, bias=False)\n",
        "        self.linear_transform = nn.Linear(self.hidden_size * 4, self.hidden_size * 2, bias=True)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.attention_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def compute_scores(self, hidden, mask):\n",
        "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
        "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n",
        "        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n",
        "        alpha = self.linear_three(torch.tanh(q1 + q2))\n",
        "        alpha = alpha.masked_fill((1.0 - mask.view(mask.shape[0], -1, 1).float()).bool(), float('-inf'))\n",
        "        alpha = self.softmax(alpha) * mask.view(mask.shape[0], -1, 1).float()\n",
        "        a = torch.sum(alpha * hidden, 1)\n",
        "        #a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n",
        "        if not self.nonhybrid:\n",
        "            a = self.linear_transform(torch.cat([a, ht], 1))\n",
        "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
        "        cat_b = self.embedding_side.weight[index_list]\n",
        "        b = torch.cat((b, cat_b), 1)\n",
        "        scores = torch.matmul(a, b.transpose(1, 0))\n",
        "        return scores, alpha[:, :, 0]\n",
        "\n",
        "    def forward(self, inputs, A, side_info, B):\n",
        "        hidden = self.embedding(inputs)\n",
        "        hidden = self.gnn(A, hidden)\n",
        "        hidden_side = self.embedding(side_info)\n",
        "        hidden_side = self.gnn(B, hidden_side)\n",
        "\n",
        "        return hidden, hidden_side\n",
        "\n",
        "\n",
        "def trans_to_cuda(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cuda()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def trans_to_cpu(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cpu()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def forward(model, i, data):\n",
        "    alias_inputs, alias_side, A, items, B, side_info, mask, targets, attention_item_target = data.get_slice(i)\n",
        "    attention_item_target = trans_to_cuda(torch.Tensor(attention_item_target).float())\n",
        "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
        "    alias_side = trans_to_cuda(torch.Tensor(alias_side).long())\n",
        "    items = trans_to_cuda(torch.Tensor(items).long())\n",
        "    side_info = trans_to_cuda(torch.Tensor(side_info).long())\n",
        "    A = trans_to_cuda(torch.Tensor(A).float())\n",
        "    B = trans_to_cuda(torch.Tensor(B).float())\n",
        "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
        "    hidden, hidden_side = model(items, A, side_info, B)\n",
        "    get = lambda i: hidden[i][alias_inputs[i]]\n",
        "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
        "    get = lambda i: hidden_side[i][alias_side[i]]\n",
        "    seq_side = torch.stack([get(i) for i in torch.arange(len(alias_side)).long()])\n",
        "    seq = torch.cat((seq_hidden, seq_side), 2)\n",
        "    scores, attention = model.compute_scores(seq, mask)\n",
        "    return targets, scores, attention, attention_item_target\n",
        "\n",
        "\n",
        "def train_test(model, train_data, test_data, attention_lambda):\n",
        "    print('start training: ', datetime.datetime.now())\n",
        "    epsilon = 1e-7\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    slices = train_data.generate_batch(model.batch_size)\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        model.optimizer.zero_grad()\n",
        "        targets, scores, attention, attention_item_target = forward(model, i, train_data)\n",
        "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
        "        attention_loss = model.attention_loss((attention + epsilon).log(), (attention_item_target + epsilon))\n",
        "        loss = model.loss_function(scores, targets - 1) + attention_lambda * attention_loss\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        total_loss += loss\n",
        "        if j % int(len(slices) / 5 + 1) == 0:\n",
        "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
        "    model.scheduler.step()\n",
        "    print('\\tLoss:\\t%.3f' % total_loss)\n",
        "\n",
        "    print('start predicting: ', datetime.datetime.now())\n",
        "    model.eval()\n",
        "    hit, mrr = [], []\n",
        "    slices = test_data.generate_batch(model.batch_size)\n",
        "    for i in slices:\n",
        "        targets, scores, _, _, = forward(model, i, test_data)\n",
        "        sub_scores = scores.topk(20)[1]\n",
        "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
        "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
        "            hit.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr.append(0)\n",
        "            else:\n",
        "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
        "    hit = np.mean(hit) * 100\n",
        "    mrr = np.mean(mrr) * 100\n",
        "    return hit, mrr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLNse_VrPjfk"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def build_graph(train_data):\n",
        "    graph = nx.DiGraph()\n",
        "    for seq in train_data:\n",
        "        for i in range(len(seq) - 1):\n",
        "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
        "                weight = 1\n",
        "            else:\n",
        "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
        "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
        "    for node in graph.nodes:\n",
        "        sum = 0\n",
        "        for j, i in graph.in_edges(node):\n",
        "            sum += graph.get_edge_data(j, i)['weight']\n",
        "        if sum != 0:\n",
        "            for j, i in graph.in_edges(i):\n",
        "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def data_masks(all_usr_pois, side_info, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    us_side = [upois + item_tail * (len_max - le) for upois, le in zip(side_info, us_lens)]\n",
        "    return us_pois, us_side, us_msks, len_max\n",
        "\n",
        "\n",
        "def split_validation(train_set, valid_portion):\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
        "\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, data, side_info, shuffle=False, graph=None):\n",
        "        inputs = data[0]\n",
        "        inputs, side, mask, len_max = data_masks(inputs, side_info, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.side = np.asarray(side)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "        self.graph = graph\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.side = self.side[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, i):\n",
        "        attention_weight = 1.0\n",
        "        inputs, side, mask, targets = self.inputs[i], self.side[i], self.mask[i], self.targets[i]\n",
        "        items, n_node, A, alias_inputs = [], [], [], []\n",
        "\n",
        "        #mask_sum = np.expand_dims(np.sum(mask, axis=1), axis=-1)\n",
        "        #default_attention_item = mask / mask_sum\n",
        "        default_attention_item = mask * 1e-6\n",
        "        default_attention_item[np.where(inputs == np.expand_dims(targets, axis=-1))] += attention_weight\n",
        "        default_attention_item_sum = np.expand_dims(np.sum(default_attention_item, axis=1), axis=-1)\n",
        "        default_attention_item = default_attention_item / default_attention_item_sum\n",
        "\n",
        "        #default_attention_side = mask / mask_sum\n",
        "        default_attention_side = mask * 1e-6\n",
        "        default_attention_side[np.where(side == np.expand_dims(index_list[(targets - 1)], axis=-1))] += attention_weight\n",
        "        default_attention_side_sum = np.expand_dims(np.sum(default_attention_side, axis=1), axis=-1)\n",
        "        default_attention_side = default_attention_side / default_attention_side_sum\n",
        "\n",
        "\n",
        "        for u_input in inputs:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in inputs:\n",
        "            node = np.unique(u_input)\n",
        "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
        "            A.append(u_A)\n",
        "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "\n",
        "        side_info, n_node, B = [], [], []\n",
        "        alias_side = []\n",
        "        for u_input in side:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in side:\n",
        "            node = np.unique(u_input)\n",
        "            side_info.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
        "            B.append(u_A)\n",
        "            alias_side.append([np.where(node == i)[0][0] for i in u_input])\n",
        "        return alias_inputs, alias_side, A, items, B, side_info, mask, targets, (default_attention_item + default_attention_side) / 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjTVSR7BJORC"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def init_seed(seed=None):\n",
        "    if seed is None:\n",
        "        seed = int(get_ms() // 1000)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "init_seed(123456)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4VnrPSzjD-v"
      },
      "source": [
        "import pickle\n",
        "index_list = pickle.load(open('/content/tafeng_item_to_side_index.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt6Rn52GB9dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201a2042-37df-4f8c-aa6e-22465d9dc358"
      },
      "source": [
        "import argparse\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', default='tafeng', help='dataset name: diginetica/yoochoose1_4/yoochoose1_64/sample')\n",
        "parser.add_argument('--batchSize', type=int, default=100, help='input batch size')\n",
        "parser.add_argument('--hiddenSize', type=int, default=100, help='hidden state size')\n",
        "parser.add_argument('--epoch', type=int, default=2, help='the number of epochs to train for')\n",
        "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')  # [0.001, 0.0005, 0.0001]\n",
        "parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n",
        "parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n",
        "parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
        "parser.add_argument('--step', type=int, default=1, help='gnn propogation steps')\n",
        "parser.add_argument('--patience', type=int, default=10, help='the number of epoch to wait before early stop ')\n",
        "parser.add_argument('--nonhybrid', action='store_true', help='only use the global preference to predict')\n",
        "parser.add_argument('--validation', action='store_true', help='validation')\n",
        "parser.add_argument('--valid_portion', type=float, default=0.1, help='split the portion of training set as validation set')\n",
        "opt, unknown = parser.parse_known_args()\n",
        "print(opt)\n",
        "\n",
        "\n",
        "def main(attention_lambda):\n",
        "    init_seed(123456)\n",
        "    train_data = pickle.load(open('tafeng_train.pkl', 'rb'))\n",
        "    train_side = pickle.load(open('tafeng_train_side.pkl', 'rb'))[0]\n",
        "    if opt.validation:\n",
        "        train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
        "        test_data = valid_data\n",
        "    else:\n",
        "        test_data = pickle.load(open('tafeng_test.pkl', 'rb'))\n",
        "        test_side = pickle.load(open('tafeng_test_side.pkl', 'rb'))[0]\n",
        "    # all_train_seq = pickle.load(open('../datasets/' + opt.dataset + '/all_train_seq.txt', 'rb'))\n",
        "    # g = build_graph(all_train_seq)\n",
        "    train_data = Data(train_data, train_side, shuffle=True)\n",
        "    test_data = Data(test_data, test_side, shuffle=False)\n",
        "    # del all_train_seq, g\n",
        "    if opt.dataset == \"movie\":\n",
        "        n_node = 7731\n",
        "    elif opt.dataset == 'diginetica':\n",
        "        n_node = 43098\n",
        "    elif opt.dataset == 'tafeng':\n",
        "        n_node = 23813\n",
        "    elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
        "        n_node = 37484\n",
        "    else:\n",
        "        n_node = 107392\n",
        "    n_side = 2013\n",
        "\n",
        "    model = trans_to_cuda(SessionGraph(opt, n_node, n_side))\n",
        "    print(11111111111)\n",
        "    print(next(model.parameters()).is_cuda)\n",
        "    print(22222222)\n",
        "\n",
        "    start = time.time()\n",
        "    best_result = [0, 0]\n",
        "    best_epoch = [0, 0]\n",
        "    bad_counter = 0\n",
        "    for epoch in range(opt.epoch):\n",
        "        print('-------------------------------------------------------')\n",
        "        print('-------------------------------------------------------')\n",
        "        print('epoch: ', epoch)\n",
        "        hit, mrr = train_test(model, train_data, test_data, attention_lambda)\n",
        "        flag = 0\n",
        "        if hit >= best_result[0]:\n",
        "            best_result[0] = hit\n",
        "            best_epoch[0] = epoch\n",
        "            flag = 1\n",
        "        if mrr >= best_result[1]:\n",
        "            best_result[1] = mrr\n",
        "            best_epoch[1] = epoch\n",
        "            flag = 1\n",
        "        print('Best Result:')\n",
        "        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
        "        bad_counter += 1 - flag\n",
        "        if bad_counter >= opt.patience:\n",
        "            break\n",
        "    print('-------------------------------------------------------')\n",
        "    end = time.time()\n",
        "    print(\"Run time: %f s\" % (end - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batchSize=100, dataset='tafeng', epoch=2, hiddenSize=100, l2=1e-05, lr=0.001, lr_dc=0.1, lr_dc_step=3, nonhybrid=False, patience=10, step=1, valid_portion=0.1, validation=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWyTDTLjIXb",
        "outputId": "00837452-c256-47d3-9cdc-81818ef33b61"
      },
      "source": [
        "for attention_lambda in [0, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]:\n",
        "    print(attention_lambda)\n",
        "    main(attention_lambda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-04 23:39:33.060877\n",
            "[0/5130] Loss: 10.0912\n",
            "[1027/5130] Loss: 8.1695\n",
            "[2054/5130] Loss: 9.0362\n",
            "[3081/5130] Loss: 8.8722\n",
            "[4108/5130] Loss: 8.4176\n",
            "\tLoss:\t44159.543\n",
            "start predicting:  2021-12-04 23:54:14.614636\n",
            "Best Result:\n",
            "\tRecall@20:\t9.1566\tMMR@20:\t2.6359\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-04 23:55:37.389492\n",
            "[0/5130] Loss: 8.0902\n",
            "[1027/5130] Loss: 8.4933\n",
            "[2054/5130] Loss: 8.1903\n",
            "[3081/5130] Loss: 8.5705\n",
            "[4108/5130] Loss: 8.2061\n",
            "\tLoss:\t42559.777\n",
            "start predicting:  2021-12-05 00:10:17.882251\n",
            "Best Result:\n",
            "\tRecall@20:\t10.1863\tMMR@20:\t2.9872\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1926.853294 s\n",
            "0.01\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 00:11:53.366315\n",
            "[0/5130] Loss: 10.0928\n",
            "[1027/5130] Loss: 8.1934\n",
            "[2054/5130] Loss: 9.0377\n",
            "[3081/5130] Loss: 8.8976\n",
            "[4108/5130] Loss: 8.4338\n",
            "\tLoss:\t44248.352\n",
            "start predicting:  2021-12-05 00:26:35.863410\n",
            "Best Result:\n",
            "\tRecall@20:\t9.0774\tMMR@20:\t2.5809\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 00:27:57.926680\n",
            "[0/5130] Loss: 8.1082\n",
            "[1027/5130] Loss: 8.5370\n",
            "[2054/5130] Loss: 8.1995\n",
            "[3081/5130] Loss: 8.5889\n",
            "[4108/5130] Loss: 8.1961\n",
            "\tLoss:\t42656.113\n",
            "start predicting:  2021-12-05 00:42:36.609263\n",
            "Best Result:\n",
            "\tRecall@20:\t10.0593\tMMR@20:\t2.9518\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1925.559882 s\n",
            "0.05\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 00:44:12.211372\n",
            "[0/5130] Loss: 10.0990\n",
            "[1027/5130] Loss: 8.2475\n",
            "[2054/5130] Loss: 9.0649\n",
            "[3081/5130] Loss: 8.9302\n",
            "[4108/5130] Loss: 8.4481\n",
            "\tLoss:\t44436.094\n",
            "start predicting:  2021-12-05 00:58:47.444507\n",
            "Best Result:\n",
            "\tRecall@20:\t8.8589\tMMR@20:\t2.5234\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 01:00:08.751301\n",
            "[0/5130] Loss: 8.1423\n",
            "[1027/5130] Loss: 8.5606\n",
            "[2054/5130] Loss: 8.2721\n",
            "[3081/5130] Loss: 8.6433\n",
            "[4108/5130] Loss: 8.2023\n",
            "\tLoss:\t42886.902\n",
            "start predicting:  2021-12-05 01:14:46.631682\n",
            "Best Result:\n",
            "\tRecall@20:\t9.8558\tMMR@20:\t2.8519\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1917.850866 s\n",
            "0.1\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 01:16:25.011577\n",
            "[0/5130] Loss: 10.1067\n",
            "[1027/5130] Loss: 8.2908\n",
            "[2054/5130] Loss: 9.0935\n",
            "[3081/5130] Loss: 8.9628\n",
            "[4108/5130] Loss: 8.4687\n",
            "\tLoss:\t44560.219\n",
            "start predicting:  2021-12-05 01:31:06.001477\n",
            "Best Result:\n",
            "\tRecall@20:\t8.6322\tMMR@20:\t2.4972\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 01:32:27.768675\n",
            "[0/5130] Loss: 8.1887\n",
            "[1027/5130] Loss: 8.5903\n",
            "[2054/5130] Loss: 8.3054\n",
            "[3081/5130] Loss: 8.6727\n",
            "[4108/5130] Loss: 8.2262\n",
            "\tLoss:\t43062.719\n",
            "start predicting:  2021-12-05 01:46:43.784348\n",
            "Best Result:\n",
            "\tRecall@20:\t9.6865\tMMR@20:\t2.8056\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1898.833185 s\n",
            "0.5\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 01:48:16.687903\n",
            "[0/5130] Loss: 10.1687\n",
            "[1027/5130] Loss: 8.4754\n",
            "[2054/5130] Loss: 9.2021\n",
            "[3081/5130] Loss: 9.0855\n",
            "[4108/5130] Loss: 8.5983\n",
            "\tLoss:\t45140.809\n",
            "start predicting:  2021-12-05 02:02:19.147110\n",
            "Best Result:\n",
            "\tRecall@20:\t8.3741\tMMR@20:\t2.4449\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 02:03:35.310021\n",
            "[0/5130] Loss: 8.3472\n",
            "[1027/5130] Loss: 8.7075\n",
            "[2054/5130] Loss: 8.4145\n",
            "[3081/5130] Loss: 8.7977\n",
            "[4108/5130] Loss: 8.3398\n",
            "\tLoss:\t43716.914\n",
            "start predicting:  2021-12-05 02:18:00.504345\n",
            "Best Result:\n",
            "\tRecall@20:\t9.3969\tMMR@20:\t2.7201\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1865.866041 s\n",
            "1\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 02:19:36.839313\n",
            "[0/5130] Loss: 10.2461\n",
            "[1027/5130] Loss: 8.6541\n",
            "[2054/5130] Loss: 9.3162\n",
            "[3081/5130] Loss: 9.1917\n",
            "[4108/5130] Loss: 8.7289\n",
            "\tLoss:\t45744.594\n",
            "start predicting:  2021-12-05 02:34:18.464180\n",
            "Best Result:\n",
            "\tRecall@20:\t8.2607\tMMR@20:\t2.3955\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 02:35:41.155558\n",
            "[0/5130] Loss: 8.4890\n",
            "[1027/5130] Loss: 8.7919\n",
            "[2054/5130] Loss: 8.5120\n",
            "[3081/5130] Loss: 8.9193\n",
            "[4108/5130] Loss: 8.4747\n",
            "\tLoss:\t44345.551\n",
            "start predicting:  2021-12-05 02:50:15.499078\n",
            "Best Result:\n",
            "\tRecall@20:\t9.2249\tMMR@20:\t2.6936\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1920.295944 s\n",
            "5\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 02:51:50.573237\n",
            "[0/5130] Loss: 10.8656\n",
            "[1027/5130] Loss: 9.9157\n",
            "[2054/5130] Loss: 10.2507\n",
            "[3081/5130] Loss: 10.0997\n",
            "[4108/5130] Loss: 9.6054\n",
            "\tLoss:\t50279.566\n",
            "start predicting:  2021-12-05 03:06:28.097649\n",
            "Best Result:\n",
            "\tRecall@20:\t7.7869\tMMR@20:\t2.2725\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 03:07:49.976160\n",
            "[0/5130] Loss: 9.5434\n",
            "[1027/5130] Loss: 9.5381\n",
            "[2054/5130] Loss: 9.1671\n",
            "[3081/5130] Loss: 9.7512\n",
            "[4108/5130] Loss: 9.4036\n",
            "\tLoss:\t48914.324\n",
            "start predicting:  2021-12-05 03:22:31.304080\n",
            "Best Result:\n",
            "\tRecall@20:\t8.8944\tMMR@20:\t2.5954\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1923.259017 s\n",
            "10\n",
            "23813\n",
            "2013\n",
            "11111111111\n",
            "True\n",
            "22222222\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2021-12-05 03:24:07.269915\n",
            "[0/5130] Loss: 11.6399\n",
            "[1027/5130] Loss: 11.4670\n",
            "[2054/5130] Loss: 11.4154\n",
            "[3081/5130] Loss: 11.2168\n",
            "[4108/5130] Loss: 10.6970\n",
            "\tLoss:\t55874.914\n",
            "start predicting:  2021-12-05 03:38:51.956315\n",
            "Best Result:\n",
            "\tRecall@20:\t7.5697\tMMR@20:\t2.2289\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2021-12-05 03:40:13.321888\n",
            "[0/5130] Loss: 10.8170\n",
            "[1027/5130] Loss: 10.4369\n",
            "[2054/5130] Loss: 10.0400\n",
            "[3081/5130] Loss: 10.7539\n",
            "[4108/5130] Loss: 10.5982\n",
            "\tLoss:\t54491.215\n",
            "start predicting:  2021-12-05 03:54:47.374129\n",
            "Best Result:\n",
            "\tRecall@20:\t8.7059\tMMR@20:\t2.5391\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "Run time: 1922.779795 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlzJELWs5uqu"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDB4qKp16ekz"
      },
      "source": [
        "weight = [0, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
        "recall = [10.1863, 10.0593, 9.8558, 9.6865, 9.3969, 9.2249, 8.8944, 8.7059]\n",
        "mrr = [2.9872, 2.9518, 2.8519, 2.8056, 2.7201, 2.6936, 2.5954, 2.5391]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "zMwjONV45xHP",
        "outputId": "867f96f2-7727-41a8-8026-74a41ac4ccd6"
      },
      "source": [
        "\n",
        "plt.plot(np.array(weight), np.array(recall),label=\"Recall@20\")\n",
        "plt.plot(np.array(weight), np.array(mrr),label=\"MRR@20\")\n",
        "plt.xlabel(\"weight of attention loss\")\n",
        "plt.ylabel(\"Metric(%)\")\n",
        "plt.title(\"Effect of attention loss: SR-GNN on TaFeng\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc04dc70ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}